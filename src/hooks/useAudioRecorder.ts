import { useState, useRef, useCallback } from 'react';
import { buildWavFromFloat } from '../services/audioEncoding';

export const useAudioRecorder = () => {
  const [isRecording, setIsRecording] = useState(false);
  const recordingStartedRef = useRef(false);
  const [isPaused, setIsPaused] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [audioBlob, setAudioBlob] = useState<Blob | null>(null);
  const [recordingMode, setRecordingMode] = useState<'microphone' | 'system' | 'visio'>('microphone');
  const [partialAudioChunks, setPartialAudioChunks] = useState<Blob[]>([]);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<number | null>(null);
  const startTimeRef = useRef<number>(0);
  const pausedTimeRef = useRef<number>(0);
  const streamRef = useRef<MediaStream | null>(null);
  const partialChunksRef = useRef<Blob[]>([]);
  const segmentQueueRef = useRef<Blob[]>([]); // File FIFO de segments MediaRecorder valides
  const lastPartialIndexRef = useRef<number>(0); // Index du dernier chunk trait√©
  const [audioStream, setAudioStream] = useState<MediaStream | null>(null);
  // WebAudio pipeline pour capture continue
  const audioContextRef = useRef<AudioContext | null>(null);
  const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const sampleRateRef = useRef<number>(16000);
  const ringBuffersRef = useRef<Float32Array[]>([]); // accumulation de frames mono

  const startRecording = useCallback(async (mode: 'microphone' | 'system' | 'visio' = 'microphone') => {
    try {
      let combinedStream: MediaStream;
      const isFirefox = navigator.userAgent.toLowerCase().includes('firefox');
      
      // D√©tecter si on est sur mobile
      const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
      console.log('üì± Appareil mobile d√©tect√©:', isMobile);

      if (mode === 'microphone') {
        // Mode microphone uniquement (comportement actuel - peut bloquer Discord/Zoom)
        combinedStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            channelCount: 1,
            sampleRate: 48000
          }
        });
      } else if (mode === 'visio') {
        // Mode Visio: capturer simultan√©ment microphone + audio syst√®me
        
        if (isMobile) {
          // üö® SUR MOBILE: getDisplayMedia n'est pas support√©
          // Solution: utiliser le microphone avec des param√®tres optimis√©s pour capturer
          // l'audio du haut-parleur (la r√©union Teams/Meet/WhatsApp)
          console.log('üì± Mode visio sur mobile: utilisation du microphone optimis√©');
          console.log('üí° Conseil: Activez le haut-parleur de votre t√©l√©phone pendant la r√©union');
          
          combinedStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              // D√©sactiver echo cancellation pour mieux capter l'audio du haut-parleur
              echoCancellation: false,
              // D√©sactiver noise suppression pour ne pas filtrer l'audio de la r√©union
              noiseSuppression: false,
              // Activer auto gain pour amplifier l'audio capt√©
              autoGainControl: true,
              channelCount: 1,
              sampleRate: 48000
            }
          });
          
          // Afficher un message √† l'utilisateur
          if (typeof window !== 'undefined' && (window as any).showMobileVisioTip) {
            (window as any).showMobileVisioTip();
          }
          
        } else {
          // üíª SUR DESKTOP: Mixer microphone + audio syst√®me
          try {
            // 1. Capturer l'audio syst√®me (√©cran/onglet)
            let systemStream: MediaStream;
            try {
              // @ts-ignore - getDisplayMedia existe mais TypeScript ne le conna√Æt pas toujours
              systemStream = await navigator.mediaDevices.getDisplayMedia({
                video: { cursor: 'always' },
                // @ts-ignore - systemAudio est support√© sur Chromium
                audio: { 
                  echoCancellation: false,
                  noiseSuppression: false,
                  autoGainControl: false,
                  systemAudio: 'include' as any 
                }
              } as any);
            } catch (_e) {
              // Fallback: simple bool√©en (Firefox/anciennes versions)
              // @ts-ignore
              systemStream = await navigator.mediaDevices.getDisplayMedia({
                video: { cursor: 'always' },
                audio: true
              });
            }
            
            // Couper la piste vid√©o imm√©diatement
            systemStream.getVideoTracks().forEach(track => track.stop());
            
            // 2. Capturer le microphone
            const micStream = await navigator.mediaDevices.getUserMedia({
              audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
                channelCount: 1,
                sampleRate: 48000
              }
            });
            
            // 3. Mixer les deux streams audio
            const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            
            // Cr√©er un gain node pour mixer
            const mixer = audioContext.createGain();
            mixer.gain.value = 1.0;
            
            // Connecter les deux sources au mixer
            const systemSource = audioContext.createMediaStreamSource(systemStream);
            const micSource = audioContext.createMediaStreamSource(micStream);
            
            systemSource.connect(mixer);
            micSource.connect(mixer);
            
            // Cr√©er un stream de destination pour l'enregistrement
            const destination = audioContext.createMediaStreamDestination();
            mixer.connect(destination);
            
            combinedStream = destination.stream;
            
            // Stocker les streams pour cleanup
            streamRef.current = systemStream;
            
          } catch (error) {
            console.error('Erreur mode visio desktop:', error);
            // Fallback: microphone uniquement
            combinedStream = await navigator.mediaDevices.getUserMedia({
              audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
                channelCount: 1,
                sampleRate: 48000
              }
            });
          }
        }
      } else if (mode === 'system') {
        // Mode audio syst√®me uniquement (capture l'onglet ou l'√©cran avec audio)
        // @ts-ignore - getDisplayMedia existe mais TypeScript ne le conna√Æt pas toujours
        combinedStream = await navigator.mediaDevices.getDisplayMedia({
          video: false,
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
      } else {
        // Mode par d√©faut: microphone uniquement
        combinedStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            channelCount: 1,
            sampleRate: 48000
          }
        });
      }

      // S√©curit√©: ne pas continuer si aucune piste audio
      if (!combinedStream || combinedStream.getAudioTracks().length === 0) {
        throw new Error('NO_AUDIO_TRACKS');
      }
      // Choisir un mimeType support√© pour des segments autonomes
      const preferredTypes = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg;codecs=opus',
        'audio/mp4',
        'audio/mpeg'
      ];
      let selectedType: string | undefined;
      for (const t of preferredTypes) {
        // @ts-ignore
        if ((window as any).MediaRecorder && (window as any).MediaRecorder.isTypeSupported && (window as any).MediaRecorder.isTypeSupported(t)) {
          selectedType = t; break;
        }
      }
      const options: MediaRecorderOptions = selectedType ? { mimeType: selectedType, audioBitsPerSecond: 128000 } : { audioBitsPerSecond: 128000 };
      const mediaRecorder = new MediaRecorder(combinedStream, options);
      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];
      partialChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          // Chaque event.data est un segment WEBM autonome (avec headers)
          chunksRef.current.push(event.data);
          partialChunksRef.current.push(event.data);
          segmentQueueRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const blob = new Blob(chunksRef.current, { type: 'audio/webm' });
        setAudioBlob(blob);
        
        // Arr√™ter tous les flux
        combinedStream.getTracks().forEach(track => track.stop());
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop());
        }
      };

      // Mettre √† jour l'√©tat imm√©diatement
      
      recordingStartedRef.current = true;
      setIsRecording(true);
      setRecordingMode(mode);
      setAudioStream(combinedStream);
      startTimeRef.current = Date.now();
      
      
      // Forcer un re-render en utilisant une fonction de callback
      setTimeout(() => {
        
        setIsRecording(prev => {
          
          return true;
        });
      }, 100);

      // WebAudio: capturer en continu en Float32 pour extraire des fen√™tres glissantes
      const AC = (window as any).AudioContext || (window as any).webkitAudioContext;
      const ac: AudioContext = new AC();
      audioContextRef.current = ac;
      sampleRateRef.current = ac.sampleRate;
      const src = ac.createMediaStreamSource(combinedStream);
      sourceNodeRef.current = src;
      const sp = ac.createScriptProcessor(4096, 1, 1);
      processorRef.current = sp;
      src.connect(sp);
      sp.connect(ac.destination);
      sp.onaudioprocess = (e: AudioProcessingEvent) => {
        const mono = e.inputBuffer.getChannelData(0);
        ringBuffersRef.current.push(new Float32Array(mono));
        // Prune m√©moire: garder ~10 minutes max
        const sr = sampleRateRef.current;
        let total = 0;
        for (let i = ringBuffersRef.current.length - 1; i >= 0 && total < sr * 600; i--) {
          total += ringBuffersRef.current[i].length;
        }
        // si plus que 10 min, retirer ancien d√©but
        while (ringBuffersRef.current.length > 0 && total > sr * 600) {
          const first = ringBuffersRef.current.shift();
          if (first) total -= first.length;
        }
      };

      // MediaRecorder pour l'archive finale uniquement
      mediaRecorder.start();

      timerRef.current = window.setInterval(() => {
        const elapsed = Math.floor((Date.now() - startTimeRef.current - pausedTimeRef.current) / 1000);
        setRecordingTime(elapsed);

        // Arr√™ter automatiquement apr√®s 4h (14400 secondes)
        if (elapsed >= 14400) {
          stopRecording();
          alert('Enregistrement automatiquement arr√™t√© apr√®s 4 heures (limite maximale)');
        }
      }, 1000);
    } catch (error) {
      // √âchec silencieux: on laisse l'UI d√©cider d'afficher un √©tat discret si besoin
      // (Plus de popup bloquante ici.)
    }
  }, []);

  const pauseRecording = useCallback(() => {
    console.log('‚è∏Ô∏è pauseRecording appel√©', {
      hasMediaRecorder: !!mediaRecorderRef.current,
      isRecording,
      isPaused
    });
    
    if (mediaRecorderRef.current && isRecording && !isPaused) {
      console.log('‚úÖ Mise en pause de l\'enregistrement confirm√©e');
      mediaRecorderRef.current.pause();
      setIsPaused(true);
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
      console.log('‚úÖ √âtat isPaused devrait maintenant √™tre true');
    } else {
      console.warn('‚ö†Ô∏è Conditions non remplies pour pause:', {
        hasMediaRecorder: !!mediaRecorderRef.current,
        isRecording,
        isPaused
      });
    }
  }, [isRecording, isPaused]);

  const resumeRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording && isPaused) {
      const pauseDuration = Date.now() - (startTimeRef.current + recordingTime * 1000);
      pausedTimeRef.current += pauseDuration;

      mediaRecorderRef.current.resume();
      setIsPaused(false);

      timerRef.current = window.setInterval(() => {
        const elapsed = Math.floor((Date.now() - startTimeRef.current - pausedTimeRef.current) / 1000);
        setRecordingTime(elapsed);

        // Arr√™ter automatiquement apr√®s 4h (14400 secondes)
        if (elapsed >= 14400) {
          stopRecording();
          alert('Enregistrement automatiquement arr√™t√© apr√®s 4 heures (limite maximale)');
        }
      }, 1000);
    }
  }, [isRecording, isPaused, recordingTime]);

  const stopRecording = useCallback(() => {
    console.log('üõë stopRecording appel√©', {
      hasMediaRecorder: !!mediaRecorderRef.current,
      isRecording,
      recordingStarted: recordingStartedRef.current
    });
    
    if (mediaRecorderRef.current && isRecording && recordingStartedRef.current) {
      console.log('‚úÖ Arr√™t de l\'enregistrement...');
      
      // Demander un flush imm√©diat du dernier chunk avant d'arr√™ter
      try {
        mediaRecorderRef.current.requestData();
      } catch (_e) {}
      mediaRecorderRef.current.stop();
      recordingStartedRef.current = false;
      setIsRecording(false);
      setIsPaused(false);
      setAudioStream(null);

      if (timerRef.current) {
        console.log('‚èπÔ∏è Arr√™t du timer d\'enregistrement');
        clearInterval(timerRef.current);
        timerRef.current = null;
      } else {
        console.warn('‚ö†Ô∏è Timer d√©j√† arr√™t√© ou non initialis√©');
      }
      // Cleanup WebAudio
      try { processorRef.current?.disconnect(); } catch {}
      try { sourceNodeRef.current?.disconnect(); } catch {}
      try { audioContextRef.current?.close(); } catch {}
      processorRef.current = null;
      sourceNodeRef.current = null;
      audioContextRef.current = null;
      
      console.log('‚úÖ Enregistrement arr√™t√© avec succ√®s');
    } else {
      console.warn('‚ö†Ô∏è stopRecording: conditions non remplies', {
        hasMediaRecorder: !!mediaRecorderRef.current,
        isRecording,
        recordingStarted: recordingStartedRef.current
      });
    }
  }, [isRecording]);

  const resetRecording = useCallback(() => {
    
    setRecordingTime(0);
    setAudioBlob(null);
    chunksRef.current = [];
    partialChunksRef.current = [];
    lastPartialIndexRef.current = 0; // Reset l'index
    pausedTimeRef.current = 0;
    setPartialAudioChunks([]);
    recordingStartedRef.current = false;
  }, []);

  const getPartialAudio = useCallback(() => {
    // Retourner TOUS les chunks accumul√©s depuis le dernier traitement r√©ussi
    // (pas seulement les nouveaux, car on a besoin d'un fichier WebM valide avec headers)
    const chunksToProcess = partialChunksRef.current.slice(lastPartialIndexRef.current);
    
    if (chunksToProcess.length === 0) {
      return null;
    }
    
    // Cr√©er un blob avec TOUS les chunks depuis le d√©but pour avoir un fichier valide
    const blob = new Blob(partialChunksRef.current, { type: 'audio/webm' });
    
    return blob;
  }, []);

  // Extrait les 15 derni√®res secondes et renvoie un WAV 16k mono pr√™t √† envoyer
  const getLast15sWav = useCallback(async (): Promise<Blob | null> => {
    const sr = sampleRateRef.current;
    const targetSamples = Math.floor(15 * sr);
    // concat√©ner depuis la fin
    if (ringBuffersRef.current.length === 0) return null;
    let needed = targetSamples;
    const chunks: Float32Array[] = [];
    for (let i = ringBuffersRef.current.length - 1; i >= 0 && needed > 0; i--) {
      const buf = ringBuffersRef.current[i];
      if (buf.length <= needed) {
        chunks.unshift(buf);
        needed -= buf.length;
      } else {
        // prendre la fin du buffer
        chunks.unshift(buf.subarray(buf.length - needed));
        needed = 0;
      }
    }
    if (chunks.length === 0) return null;
    let total = 0; for (const c of chunks) total += c.length;
    const merged = new Float32Array(total);
    let off = 0; for (const c of chunks) { merged.set(c, off); off += c.length; }
    return await buildWavFromFloat(merged, sr);
  }, []);
  
  // R√©cup√®re le prochain segment autonome pr√™t √† la transcription (FIFO)
  const getNextSegmentAudio = useCallback(() => {
    if (segmentQueueRef.current.length === 0) return null;
    const blob = segmentQueueRef.current.shift() || null;
    return blob ?? null;
  }, []);
  
  const markPartialAsProcessed = useCallback(() => {
    // Marquer les chunks actuels comme trait√©s
    lastPartialIndexRef.current = partialChunksRef.current.length;
  }, []);

  return {
    isRecording,
    isPaused,
    recordingTime,
    audioBlob,
    recordingMode,
    partialAudioChunks,
    audioStream,
    startRecording,
    pauseRecording,
    resumeRecording,
    stopRecording,
    resetRecording,
    setRecordingMode,
    getPartialAudio,
    getNextSegmentAudio,
    getLast15sWav,
    markPartialAsProcessed,
  };
};
